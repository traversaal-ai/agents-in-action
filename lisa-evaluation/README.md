# ğŸ“Š LISA Evaluation Workflow

## Automated Evaluation Pipeline for AI Agents in n8n

The **Lisa Evaluation Workflow** is a demonstration workflow designed to showcase how to evaluate AI agents in n8n using the native Evaluation framework. It builds on the original [**LISA â€“ Your First Agent**](https://github.com/traversaal-ai/agents-in-action/tree/main/Lisa-your-first-agent) workflow and introduces dataset-driven testing, automated metric calculation, and reproducible evaluation runs.

This workflow is ideal for classes, workshops, and teams looking to understand how to systematically assess AI agent quality instead of relying on manual testing.

---

## ğŸ¯ Purpose

This workflow is part of our hands-on teaching material on **Agentic AI & n8n Evaluations**. It demonstrates how to:
- Evaluate an AI agent against a **ground-truth dataset**
- Run **batch evaluations** using Google Sheets
- Compare **expected vs actual answers**
- Automatically compute **evaluation metrics** using an LLM
- Teach best practices for **LLM evaluation in production workflows**

---

## ğŸŒŸ Key Features

- ğŸ“„ **Dataset-Driven Evaluation** using Google Sheets
- ğŸ¤– **Same Agent, Different Context** â€“ Reuses Lisa with evaluation constraints
- ğŸ§  **Short, Deterministic Answers** for easier comparison
- ğŸŒ **Tool-Aware Agent** (Internet Search + Traversaal Pro RAG)
- ğŸ“Š **Automated Metrics** using n8n Evaluation nodes
- ğŸ” **Repeatable & Scalable** evaluation runs
- ğŸ“ **Perfect for Teaching** n8n Evaluations

---

## ğŸ”„ How It Works

```bash
Google Sheets Dataset
        â†“
Evaluation Trigger
        â†“
Set Inputs (question)
        â†“
Lisa Agent
  â”œâ”€â”€ Internet Search (Ares)
  â”œâ”€â”€ Traversaal Pro (AWS Docs)
  â”œâ”€â”€ Memory (per user)
        â†“
Set Outputs (actual_answer)
        â†“
Set Metrics (LLM-based evaluation)
```

---

## ğŸ—ï¸ Architecture

![LISA Evaluation Workflow](images/lisa-evaluation.png)

---

## ğŸ§ª Evaluation Dataset

The workflow uses a **dummy evaluation dataset** stored in Google Sheets.

### Dataset Schema
| Column Name |	Description |
| ----------- | ----------- |
| `username` |	Unique user/session identifier |
| `query` |	Question sent to the agent |
| `ground_truth` |	Expected correct answer |
| `actual_answer` |	Filled automatically by workflow |

### Sample Rows
| username |	query |	ground_truth |
| user_1 |	What is Amazon S3 and what is it used for?	| S3 is a fast, scalable, and durable object-storage service used to store and retrieve any type and any amount of data. |
| user_5 |	What is the maximum execution time for Lambda functions? |	The maximum execution time for a Lambda function is 15 minutes. |
| user_10 |	What are DynamoDB secondary indexes used for?	| Secondary indexes provide the ability to query a table without using the primary key. |

### ğŸ“Œ Note:
`actual_answer` is populated automatically during evaluation runs.

---

## âš™ï¸ Workflow Components

#### 1ï¸âƒ£ Evaluation Trigger
- Reads one row at a time from Google Sheets
- Enables **batch evaluation**
- Each row becomes a single test case

### 2ï¸âƒ£ Set Inputs
- Maps dataset fields to agent inputs
- Passes `query` â†’ `question` for Lisa

### 3ï¸âƒ£ Lisa (Evaluation Mode)
- Same agent logic as production
- **System prompt constrained**:
    - *â€œAnswer the question in just one short sentence.â€*
- This improves:
    - Determinism
    - Metric reliability
    - Reduced hallucinations

### 4ï¸âƒ£ Tools Used by Lisa
- ğŸŒ **Ares Internet Search** â€“ for general knowledge
- ğŸ“š **Traversaal Pro RAG** â€“ for AWS documentation
- ğŸ§  **Memory Buffer** â€“ session-based using username

## 5ï¸âƒ£ Set Outputs
- Captures Lisaâ€™s response
- Writes it back to Google Sheets as `actual_answer`

## 6ï¸âƒ£ Set Metrics
- Compares:
    - `ground_truth`
    - `actual_answer`
- Uses **GPT-4.1-mini** as an evaluation judge
- Enables semantic evaluation beyond string matching

---

## ğŸ› ï¸ Setup Instructions

### Prerequisites
- n8n (Cloud or Self-Hosted)
- OpenAI API Key
- Traversaal API Access
- Google Sheets account

### 1ï¸âƒ£ Import the Workflow
1. Copy the **Lisa Evaluation Workflow JSON**
2. In n8n â†’ **Import Workflow**
3. Save as `Lisa Evaluation Workflow`

### 2ï¸âƒ£ Configure Credentials
**OpenAI**

- Add OpenAI credentials in n8n
- Used by:
    - Lisa Agent
    - Metrics Evaluation Model
  
**Google Sheets**

- Connect Google Sheets OAuth
- Ensure read/write access to the dataset

**Traversaal APIs**

- Ares Internet Search
```bash
{
  "name": "x-api-key",
  "value": "your_ares_api_key"
}
```

- Traversaal Pro RAG
```bash
{
  "name": "Authorization",
  "value": "Bearer your_traversaal_token"
}
```

### 3ï¸âƒ£ Link the Dataset
- Open **Evaluation Trigger**
- Select:
    - Google Sheet document
    - Sheet name
- Ensure columns match dataset schema

---

## â–¶ï¸ Running an Evaluation
1. Open **Lisa Evaluation Workflow**
2. Click **Execute Workflow**
3. n8n will:
    1. Fetch each dataset row
    2. Run Lisa
    3. Store actual answers
    4. Compute evaluation metrics
4. Results appear in:
    - Google Sheets
    - n8n execution logs

---

## ğŸ“ˆ What This Teaches

This workflow is intentionally designed for **learning**:
- Why **evaluation datasets** matter
- How to separate **agent logic from evaluation**
- How to avoid brittle string matching
- How to scale AI testing beyond manual prompts
- How n8n Evaluations fit into real AI systems

---

## ğŸ”§ Customization Ideas
- Add more columns (difficulty, category, source)
- Swap AWS docs with your own RAG corpus
- Add multiple agents for A/B testing
- Introduce grading rubrics (correctness, clarity)
- Store metrics in a database instead of Sheets

---

## ğŸ” Best Practices Highlighted

- Keep evaluation prompts **short & constrained**
- Use **semantic metrics**, not exact matches
- Separate **production** and **evaluation** workflows
- Always version your datasets
- Re-run evaluations after prompt or model changes

---

## ğŸ› Troubleshooting

- **No answers written** â†’ Check Google Sheets write permissions
- **Wrong tool usage** â†’ Inspect agent reasoning logs
- **Low scores** â†’ Review prompt constraints and ground truth
- **API errors** â†’ Validate keys and quotas
Enable **debug mode** in n8n for deeper inspection.

---

## ğŸ¤ Contributing

Contributions are welcome!

- Improve evaluation metrics
- Add multi-agent comparison
- Extend dataset templates
- Create visualization workflows
- Submit PRs with clear explanations.

---

## ğŸ“ Ready to Level-Up?

Join our courses on Maven and never stop learning:
- ğŸ¤– [Agentic AI System Design for PMs â€” _For Leaders, Managers & Career Builders_](https://maven.com/boring-bot/ml-system-design?promoCode=201OFF)
- ğŸ’» [Agent Engineering Bootcamp: Developers Edition â€” _For Developers, Engineers & Researchers_](https://maven.com/boring-bot/advanced-llm?promoCode=200OFF)

---

## Happy Evaluating! ğŸš€
